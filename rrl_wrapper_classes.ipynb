{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a wrapper for a command line operation using Robospect and Ken Carrell's \n",
    "# normalization script to\n",
    "\n",
    "# A. REPRODUCE our steps for finding a, b, c, d\n",
    "# B. APPLY our solution to any other spectra to find [Fe/H]\n",
    "\n",
    "# written by E.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from subprocess import call\n",
    "from subprocess import Popen\n",
    "import shlex\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pylab import * \n",
    "import glob\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class norm_spec:\n",
    "    \n",
    "    ##############################################################################\n",
    "    # STEP 1: NORMALIZE SPECTRA (applicable to A and B)\n",
    "    ##############################################################################\n",
    "    \n",
    "    # this is a superclass that will be inherited by other classes\n",
    "    \n",
    "    def __init__(self, input_file):\n",
    "        self.smoothing = 22 # smoothing applied by Carrell's normalization code\n",
    "        self.input_file = input_file\n",
    "        \n",
    "    def __call__(self):\n",
    "        \n",
    "        # compile Carrell's normalization code\n",
    "        # (see carrell_readme.txt)\n",
    "        normzn_compile1 = shlex.split(\"g++ -o bkgrnd bkgrnd.cc\")\n",
    "        normzn_compile2 = subprocess.Popen(normzn_compile1) # run\n",
    "        \n",
    "        # run the normalization routine on the data\n",
    "        normzn_run1 = shlex.split(\"./bkgrnd --smooth \"+str(self.smoothing)+\" \"+self.input_file)\n",
    "        normzn_run2 = subprocess.Popen(normzn_run1, stdout=subprocess.PIPE, stderr=subprocess.PIPE) # run and capture output\n",
    "                                  \n",
    "        # make list of all output files\n",
    "        dir_name = \"test_output/\"\n",
    "        list_output_files = [name for name in os.listdir(dir_name) if os.path.isfile(os.path.join(dir_name, name))]\n",
    "                                  \n",
    "        # divide the second column of the output files (empirical) with the third (normalization) \n",
    "        header = ['WAVELENGTH', 'FLUX'] # headers of output file that Robospect will use (see Robospect user manual)\n",
    "        for filenum in range(0,len(list_output_files)):\n",
    "            df = pd.read_csv('test_output/'+str(list_output_files[filenum]), delim_whitespace=True, header=None)\n",
    "            df['WAVELENGTH'] = df[0]\n",
    "            df['FLUX'] = np.divide(df[1],df[2]) # normalize\n",
    "            df.to_csv('test_normzed_output/output.csv', columns = header, index = False, sep = ' ') # write out file \n",
    "            del df                          \n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example: run normalization of raw data\n",
    "\n",
    "do_normzn = norm_spec(\"input_file\") # initialize class instance\n",
    "# do_normzn.smoothing = 22 # can overload default smoothing\n",
    "do_normzn() # call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class generate_synthetic_spec(norm_spec):\n",
    "    \n",
    "    ##############################################################################\n",
    "    # STEP 2: TAKE EMPIRICAL SPECTRA AND CHANGE NOISE CHARACTERISTICS TO GENERATE SYNTHETIC SPECTRA\n",
    "    # THIS WILL INVOLVE RUNNING RW'S FORTRAN SCRIPT (applicable to A)\n",
    "    ##############################################################################\n",
    "    \n",
    "    # inherits norm_spec class for normalizing spectra\n",
    "    \n",
    "    # normalize the synthetic spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subprocess.call([\"gfortran\",\"-o\",\"hello\",\"junk.f90\"]) # create\n",
    "#subprocess.call([\"./hello\"])   # subprocess.check_output([\"./hello\"]) lets you see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_robospect():\n",
    "### THIS NOT WORKING!!!\n",
    "\n",
    "    ##############################################################################\n",
    "    # STEP 3: RUN ROBOSPECT ON ANY SPECTRA AND WRITE OUT EW VALUES AS *.c.dat FILES (applicable to A and B)\n",
    "    ##############################################################################\n",
    "    \n",
    "    # for applying to synthetic spectra \n",
    "\n",
    "    # accumulate list of filenames of normalized synthetic spectra\n",
    "    fileNameList = glob.glob(\"../*_*.c.dat\") # (or search whatever other directory the *.c.dat files are in)\n",
    "\n",
    "    # for-loop to write out *.robolines and *.robospect files\n",
    "    for p in fileNameList: \n",
    "        # default command: (-F: find all lines; -P: sets path of output files)\n",
    "        # robospect -F -P rs.out example.dat\n",
    "        args = ['./src/robospect', '-F', '-P', 'rs.out', p]\n",
    "        q = subprocess.call(args, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N.b. Wrapper should catch anything that robospect tries to print to terminal \n",
    "# (3 pipes in any process: StdIn, StdOut, and StdError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class scraper():\n",
    "\n",
    "    ##############################################################################\n",
    "    # STEP 3B: SCRAPE ALL THE EW INFO FROM *.c.dat FILES (applicable to A and B)\n",
    "    ##############################################################################\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # directory containing the directory containing *.c.dat files\n",
    "        self.stem = '/home/../../media/unasemaje/Seagate Expansion Drive/rrlyrae_data_reduction/'\n",
    "        # subdirectory containing the *.c.dat files\n",
    "        self.subdir = 'McDrealiz'\n",
    "        \n",
    "        # get list of filenames without the path\n",
    "        fileListLong = glob.glob(stem+subdir+'/'+'*.fits.robolines')\n",
    "        fileListUnsorted = [os.path.basename(x) for x in fileListLong]\n",
    "        fileList = sorted(fileListUnsorted)\n",
    "        \n",
    "    def __call__(self):\n",
    "        \n",
    "        # sanity check: are the lines listed in order?\n",
    "        def line_check(lineCenters):\n",
    "            if ((lineCenters[0] < 3933.660-10) or (lineCenters[0] > 3933.660+10)): # CaIIK\n",
    "            print('Lines not matching!')\n",
    "            sys.exit  # ... and abort\n",
    "        elif ((lineCenters[1] < 3970.075-10) or (lineCenters[1] > 3970.075+10)): # H-epsilon (close to CaIIH)\n",
    "            print('Lines not matching!')\n",
    "            sys.exit\n",
    "        elif ((lineCenters[2] < 4101.7100-10) or (lineCenters[2] > 4101.7100+10)): # H-delta\n",
    "            print('Lines not matching!')\n",
    "            sys.exit\n",
    "        elif ((lineCenters[3] < 4340.472-10) or (lineCenters[3] > 4340.472+10)): # H-gamma\n",
    "            print('Lines not matching!')\n",
    "            sys.exit\n",
    "        elif ((lineCenters[4] < 4861.290-10) or (lineCenters[4] > 4861.290+10)): # H-beta\n",
    "            print('Lines not matching!')\n",
    "            sys.exit\n",
    "        return\n",
    "    \n",
    "    \n",
    "        # loop over all filenames, extract line data\n",
    "        for t in range(0,len(fileList)):\n",
    "        \n",
    "            # read in Robospect output\n",
    "            df = pd.read_csv(stem+subdir+'/'+fileList[t], header=13, delim_whitespace=True, index_col=False, usecols=np.arange(17))\n",
    "    \n",
    "            # check lines are in the right order\n",
    "            line_check(df['#x0'])\n",
    "    \n",
    "            # add two cols on the left: the filename, and the name of the line\n",
    "            sLength = len(df['mean']) # number of lines (should be 5)\n",
    "            df['file_name'] = pd.Series(fileList[t], index=df.index)\n",
    "            df['synth_spec_name'] = pd.Series(fileList[t].split(\".\")[0], index=df.index) # multiple synthetic spectra correspond to one empirical spectrum\n",
    "            df['empir_spec_name'] = pd.Series(fileList[t].split(\".\")[0][0:-4], index=df.index) # empirical spectrum\n",
    "            #df['star_name'] = pd.Series(fileList[t].split(\"__\")[0], index=df.index)\n",
    "            df['line_name'] = ['CaIIK', 'Heps', 'Hdel', 'Hgam', 'Hbet']\n",
    "    \n",
    "            # get an idea of the progress\n",
    "            clear_output(wait=True)\n",
    "            print('Out of '+str(len(fileList))+' files, '+str(t)+' scraped...')\n",
    "    \n",
    "            # if this is the first list, start a master copy from it to concatenate stuff to it\n",
    "            if (t==0):\n",
    "                dfMaster = df.copy()\n",
    "            else:\n",
    "                dfMaster = pd.concat([dfMaster,df])\n",
    "                del df # clear variable\n",
    "                \n",
    "        # write to csv, while resetting the indices\n",
    "        # note THIS TABLE INCLUDES ALL DATA, GOOD AND BAD\n",
    "        dfMaster_reset = dfMaster.reset_index(drop=True).copy() # this gets shown further down in this notebook\n",
    "        dfMaster.reset_index(drop=True).to_csv(stem+subdir+'/'+subdir+'_largeTable_test.csv') # this is effectively the same, but gets written out\n",
    "        \n",
    "        ## IF WE ARE INTERESTED IN SPECTRA THAT HAVE ALL WELL-FIT LINES\n",
    "        # identify the synthetic spectrum names which have at least one line with a bad fit\n",
    "        badSynthSpectra = dfMaster_reset['synth_spec_name'][np.squeeze(whereRedFlag)]\n",
    "        # remove duplicate names\n",
    "        badSynthSpectra_uniq = badSynthSpectra.drop_duplicates()\n",
    "        # keep only the spectra that have all lines well-fit\n",
    "        dfMaster_reset_dropBadSpectra = dfMaster_reset.where(~dfMaster_reset['synth_spec_name'].isin(badSynthSpectra_uniq))\n",
    "        \n",
    "        # write to csv\n",
    "        # note THIS TABLE HAS SPECTRA WITH ANY BAD ROWS REMOVED\n",
    "        dfMaster_reset_dropBadSpectra.to_csv(subdir+'_largeTable_bad_spectra_removed_test.csv') # this is effectively the same, but gets written out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example: run the scraper \n",
    "\n",
    "do_scrape = scraper() # initialize class instance\n",
    "do_scrape() # call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class findHK():\n",
    "    \n",
    "    ##############################################################################\n",
    "    # STEP 4: READ IN ROBOSPECT EWS OF SYNTHETIC SPECTRA, RESCALE THEM, AVERAGE THEM, PLOT H-K SPACE (applicable to A and B)\n",
    "    ##############################################################################\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        # read in line data\n",
    "        line_data = pd.read_csv('McDrealiz_largeTable_2017jan21_bad_spectra_removed.csv', delim_whitespace=False)\n",
    "        \n",
    "        # initialize arrays: essential info\n",
    "        empir_spec_name_array = []\n",
    "        star_name_array = []\n",
    "        H_data_array = []\n",
    "        K_data_array = []\n",
    "        err_H_data_array = [] \n",
    "        err_K_data_array = []\n",
    "\n",
    "        # initialize arrays: other info\n",
    "        Hbet_data_array = []\n",
    "        err_Hbet_data_array = []\n",
    "        Hgam_data_array = []\n",
    "        err_Hgam_data_array = []\n",
    "        rHgam_data_array = [] # rescaled Hgamma\n",
    "        err_rHgam_data_array = []\n",
    "        Hdel_data_array = []\n",
    "        err_Hdel_data_array = []\n",
    "        Heps_data_array = []\n",
    "        err_Heps_data_array = []\n",
    "        \n",
    "    def __call__(self):\n",
    "        \n",
    "        # make a list of all UNIQUE, EMPIRICAL spectrum names\n",
    "        uniqueSpecNames = line_data.drop_duplicates(subset='empir_spec_name')['empir_spec_name']\n",
    "        \n",
    "        # fit a straight line to Hgam vs Hdel\n",
    "        x_data = line_data['EQW'].where(line_data['line_name'] == 'Hdel').dropna() # Hdel\n",
    "        y_data = line_data['EQW'].where(line_data['line_name'] == 'Hgam').dropna() # Hgam\n",
    "        Hgam = np.copy(y_data)\n",
    "        m,b = polyfit(x_data, y_data, 1) # might want errors later, too \n",
    "        \n",
    "        # generate a rescaled Hgam, call it rHgam\n",
    "        rHgam_all = np.divide(np.subtract(Hgam,b),m)\n",
    "        \n",
    "        # prepare data for a plot\n",
    "        # loop over every EMPIRICAL spectrum and assemble SYNTHETIC data into arrays\n",
    "        for p in range(0,len(uniqueSpecNames)):\n",
    "    \n",
    "            # the name of the empirical spectrum being used here\n",
    "            print(np.array(uniqueSpecNames)[p])\n",
    "    \n",
    "            # extract all synthetic data corresponding to this empirical spectrum\n",
    "            data_for_this_empir_spectrum = line_data.where(line_data['empir_spec_name'][0:-4] == np.array(uniqueSpecNames)[p])\n",
    "    \n",
    "            # scrape data\n",
    "            raw_Hbet_data = data_for_this_empir_spectrum['EQW'].where(line_data['line_name'] == 'Hbet')\n",
    "            raw_Hgam_data = data_for_this_empir_spectrum['EQW'].where(line_data['line_name'] == 'Hgam')\n",
    "            raw_Hdel_data = data_for_this_empir_spectrum['EQW'].where(line_data['line_name'] == 'Hdel')\n",
    "            raw_Heps_data = data_for_this_empir_spectrum['EQW'].where(line_data['line_name'] == 'Heps')\n",
    "            raw_K_data = data_for_this_empir_spectrum['EQW'].where(line_data['line_name'] == 'CaIIK')\n",
    "    \n",
    "            # rescale and remove nans\n",
    "            Hbet_data_wnans = np.array(np.copy(raw_Hbet_data))\n",
    "            Hgam_data_wnans = np.array(np.copy(raw_Hgam_data))\n",
    "            Hdel_data_wnans = np.array(np.copy(raw_Hdel_data))\n",
    "            Heps_data_wnans = np.array(np.copy(raw_Heps_data))    \n",
    "            K_data_wnans = np.array(np.copy(raw_K_data))\n",
    "            rHgam_data_wnans = np.array(np.divide(np.subtract(raw_Hgam_data,b),m)) # rescale Hgam EWs\n",
    "    \n",
    "            Hbet_data = Hbet_data_wnans[np.isfinite(Hbet_data_wnans)] # remove nans\n",
    "            Hgam_data = Hgam_data_wnans[np.isfinite(Hgam_data_wnans)]\n",
    "            Hdel_data = Hdel_data_wnans[np.isfinite(Hdel_data_wnans)]\n",
    "            Heps_data = Heps_data_wnans[np.isfinite(Heps_data_wnans)]\n",
    "            rHgam_data = rHgam_data_wnans[np.isfinite(rHgam_data_wnans)]\n",
    "            K_data = K_data_wnans[np.isfinite(K_data_wnans)]\n",
    "    \n",
    "            # get the H-K synthetic data together\n",
    "            balmer_data_allsynthetic_spec = np.mean([Hdel_data,rHgam_data], axis=0) # Balmer EW = 0.5*(Hdel + rHgam)\n",
    "            K_data_allsynthetic_spec = np.copy(K_data)\n",
    "    \n",
    "            # the actual points to plot (or record in a table)\n",
    "            Hbet_data_pt = np.nanmedian(Hbet_data)\n",
    "            Hgam_data_pt = np.nanmedian(Hgam_data)\n",
    "            rHgam_data_pt = np.nanmedian(rHgam_data)\n",
    "            Hdel_data_pt = np.nanmedian(Hdel_data)\n",
    "            Heps_data_pt = np.nanmedian(Heps_data)\n",
    "            balmer_data_pt = np.nanmedian(balmer_data_allsynthetic_spec)\n",
    "            K_data_pt = np.nanmedian(K_data_allsynthetic_spec)\n",
    "    \n",
    "            # the error bars\n",
    "            err_Hbet_data = np.nanstd(Hbet_data)\n",
    "            err_Hgam_data = np.nanstd(Hgam_data)\n",
    "            err_rHgam_data = np.nanstd(rHgam_data)\n",
    "            err_Hdel_data = np.nanstd(Hdel_data)\n",
    "            err_Heps_data = np.nanstd(Heps_data)\n",
    "            err_balmer_data = np.nanstd(balmer_data_allsynthetic_spec)\n",
    "            err_K_data = np.nanstd(K_data_allsynthetic_spec)\n",
    "    \n",
    "            #plt.plot(balmer_data_pt,K_data_pt)\n",
    "            #plt.errorbar(balmer_data_pt, K_data_pt, yerr=err_K_data, xerr=err_balmer_data)\n",
    "\n",
    "            # append data to arrays: essential info\n",
    "            empir_spec_name_array = np.append(empir_spec_name_array,np.array(uniqueSpecNames)[p])\n",
    "            star_name_array = np.append(star_name_array,str(np.array(uniqueSpecNames)[p])[0:-3])\n",
    "            H_data_array = np.append(H_data_array,balmer_data_pt)\n",
    "            err_H_data_array = np.append(err_H_data_array,err_balmer_data)\n",
    "            K_data_array = np.append(K_data_array,K_data_pt)\n",
    "            err_K_data_array = np.append(err_K_data_array,err_K_data)\n",
    "    \n",
    "            # append data to arrays: other info\n",
    "            Hbet_data_array = np.append(Hbet_data_array,Hbet_data_pt)\n",
    "            err_Hbet_data_array = np.append(err_Hbet_data_array,err_Hbet_data)\n",
    "            Hgam_data_array = np.append(Hgam_data_array,Hgam_data_pt)\n",
    "            err_Hgam_data_array = np.append(err_Hgam_data_array,err_Hgam_data)\n",
    "            rHgam_data_array = np.append(rHgam_data_array,err_rHgam_data) # rescaled Hgamma\n",
    "            err_rHgam_data_array = np.append(err_rHgam_data_array,err_rHgam_data)\n",
    "            Hdel_data_array = np.append(Hdel_data_array,Hdel_data_pt)\n",
    "            err_Hdel_data_array = np.append(err_Hdel_data_array,err_Hdel_data)\n",
    "            Heps_data_array = np.append(Heps_data_array,Heps_data_pt)\n",
    "            err_Heps_data_array = np.append(err_Heps_data_array,err_Heps_data)\n",
    "    \n",
    "            # clear some variables\n",
    "            balmer_data_allsynthetic_spec=None \n",
    "            K_data_allsynthetic_spec=None\n",
    "            balmer_data_allsynthetic_spec=None \n",
    "            K_data_allsynthetic_spec=None\n",
    "            \n",
    "        # put everything into a dataframe\n",
    "\n",
    "        d = {'empir_spec_name': empir_spec_name_array, \n",
    "             'star_name': star_name_array,\n",
    "             'Hbet': Hbet_data_array,\n",
    "             'err_Hbet': err_Hbet_data_array,\n",
    "             'Hgam': Hgam_data_array,\n",
    "             'err_Hgam': err_Hgam_data_array,\n",
    "             'Hdel': Hdel_data_array,\n",
    "             'err_Hdel': err_Hdel_data_array,\n",
    "             'Heps': Heps_data_array,\n",
    "             'err_Heps': err_Heps_data_array, \n",
    "             'rHgam': rHgam_data_array,\n",
    "             'err_rHgam': err_rHgam_data_array,  \n",
    "             'balmer': H_data_array,\n",
    "             'err_balmer': err_H_data_array,\n",
    "             'K': K_data_array,\n",
    "             'err_K': err_K_data_array\n",
    "            }     \n",
    "        df_collation = pd.DataFrame(data=d)\n",
    "        \n",
    "        # read in a text file containing phase information\n",
    "        phase_info = pd.read_csv(\"~/Documents/PythonPrograms/all_Python_code/2016_08_27_rrlyrae_metal_fit_emcee_wrapper/eckhart_2ndPass_allSNR_noVXHer_lowAmpPrior.csv\")\n",
    "        \n",
    "        # paste phase info into the table of EWs\n",
    "        phase_array = []\n",
    "        feh_array = []\n",
    "        err_feh_array = []\n",
    "        name_array = []\n",
    "\n",
    "        for q in range(0,len(df_collation)):\n",
    "            name_this_one = phase_info['Spectrum'].where(phase_info['Spectrum'] == df_collation['empir_spec_name'][q]).dropna()\n",
    "            phase_this_one = phase_info['phase'].where(phase_info['Spectrum'] == df_collation['empir_spec_name'][q]).dropna()\n",
    "            feh_this_one = phase_info['FeH'].where(phase_info['Spectrum'] == df_collation['empir_spec_name'][q]).dropna()\n",
    "            err_feh_this_one = phase_info['eFeH'].where(phase_info['Spectrum'] == df_collation['empir_spec_name'][q]).dropna()\n",
    "            name_array = np.append(name_array,name_this_one)\n",
    "            phase_array = np.append(phase_array,phase_this_one)\n",
    "            feh_array = np.append(feh_array,feh_this_one)\n",
    "            err_feh_array = np.append(err_feh_array,err_feh_this_one)\n",
    "        df_collation_real = df_collation.dropna().copy(deep=True) # drop row of nans\n",
    "        df_collation_real['phase'] = phase_array\n",
    "        df_collation_real['FeH'] = feh_array\n",
    "        df_collation_real['eFeH'] = err_feh_array\n",
    "        \n",
    "        # write to csv\n",
    "        df_collation_real.to_csv('more_realistic_EWs_w_phase_test.csv')\n",
    "        \n",
    "        # make plot: each color is a different star, open circles are bad phase region\n",
    "        data_to_plot = pd.read_csv('more_realistic_EWs_w_phase.csv') # read data back in\n",
    "        \n",
    "        # make list of unique star names \n",
    "        unique_star_names = data_to_plot.drop_duplicates(subset=['star_name'])['star_name'].values\n",
    "        \n",
    "        # plot data points\n",
    "        cmap = plt.get_cmap(name='jet')\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        # loop over every star, overlay the set of points for that star on the plot\n",
    "        for y in range(0,len(unique_star_names)):\n",
    "    \n",
    "            x_data = data_to_plot['balmer'].where(data_to_plot['star_name'] == unique_star_names[y])\n",
    "            y_data = data_to_plot['K'].where(data_to_plot['star_name'] == unique_star_names[y])\n",
    "    \n",
    "            err_x_data = data_to_plot['err_balmer'].where(data_to_plot['star_name'] == unique_star_names[y])\n",
    "            err_y_data = data_to_plot['err_K'].where(data_to_plot['star_name'] == unique_star_names[y])\n",
    "    \n",
    "            # plot, and keep the same color for each star\n",
    "            color_this_star = cmap(float(y)/len(unique_star_names))\n",
    "            ax.errorbar(x_data,y_data,yerr=err_y_data,xerr=err_x_data,linestyle='',fmt='o',markerfacecolor=color_this_star,color = color_this_star)\n",
    "    \n",
    "            x_data_badPhase = x_data.where(np.logical_or(data_to_plot['phase'] > 0.8, data_to_plot['phase'] < 0.05))\n",
    "            y_data_badPhase = y_data.where(np.logical_or(data_to_plot['phase'] > 0.8, data_to_plot['phase'] < 0.05))\n",
    "    \n",
    "            # overplot unfilled markers to denote bad phase region\n",
    "            ax.errorbar(x_data_badPhase,y_data_badPhase,linestyle='',fmt='o',markerfacecolor='white',color = color_this_star)\n",
    "    \n",
    "            # add star name\n",
    "            ax.annotate(unique_star_names[y], xy=(np.array(x_data.dropna())[0], \n",
    "                                          np.array(y_data.dropna())[0]), \n",
    "                xytext=(np.array(x_data.dropna())[0], np.array(y_data.dropna())[0]))\n",
    "    \n",
    "        plt.title('KH plot, using synthetic spectra')\n",
    "        plt.ylabel('CaIIK EW (milliangstrom)')\n",
    "        plt.xlabel('Balmer EW (milliangstrom)')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example: run the findHK\n",
    "\n",
    "do_HK = findHK() # initialize class instance\n",
    "do_HK() # call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class run_emcee():\n",
    "    \n",
    "    ##############################################################################\n",
    "    # STEP 5: RUN EMCEE ON THE SPACE, GET VALUES FOR a, b, c, d (applicable only to A)\n",
    "    ##############################################################################\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "    def __call__(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lit_metallicities():\n",
    "    \n",
    "    ##############################################################################\n",
    "    # STEP X: READ IN LITERATURE METALLICITY VALUES AND RESCALE\n",
    "    ##############################################################################\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        stem = \"~/Documents/PythonPrograms/all_Python_code/2018_03_31_rrlyrae_rescale_a_la_chadid/\"\n",
    "        \n",
    "        # Fe/H from Layden+ 1994\n",
    "        layden_feh = pd.read_csv(stem + \"layden_1994_abundances.dat\",delimiter=';')\n",
    "        # RES: \"rather low\"\n",
    "        \n",
    "        # Fe/H Clementini+ 1995\n",
    "        clementini_feh = pd.read_csv(stem + \"clementini_1995_abundances.dat\")\n",
    "\n",
    "        # Fe/H Fernley+ 1996\n",
    "        fernley_feh = pd.read_csv(stem + \"fernley_1996_abundances.dat\")\n",
    "        # RES: 60,000, FeI & FeII, 5900-8100 A\n",
    "        \n",
    "        # log(eps) from Lambert+ 1996\n",
    "        lambert_logeps = pd.read_csv(stem + \"lambert_1996_abundances.dat\")\n",
    "        # RES: ~23,000, FeII + photometric models, 3600-9000 A\n",
    "        \n",
    "        # Fe/H from Wallerstein and Huang 2010, arXiv 1004.2017\n",
    "        wallerstein_feh = pd.read_csv(stem + \"wallerstein_huang_2010_abundances.dat\")\n",
    "        # RES: ~30,000, FeII\n",
    "        \n",
    "        # Fe/H from Chadid+ 2017 (FeI and II lines)\n",
    "        chadid_feh = pd.read_csv(stem + \"chadid_2017_abundances.dat\")\n",
    "        # RES: 38000, FeI & FeII, 3400-9900 A\n",
    "\n",
    "        # Fe/H from Liu+ 2013\n",
    "        liu_feh = pd.read_csv(stem + \"liu_2013_abundances.dat\")\n",
    "        # RES: ~60,000, FeI (& FeII?), 5100-6400 A\n",
    "\n",
    "        # Fe/H from Nemec+ 2013\n",
    "        nemec_feh = pd.read_csv(stem + \"nemec_2013_abundances.dat\")\n",
    "        # RES: ~65,000 or 36,000, FeI & FeII, 5150-5200 A\n",
    "\n",
    "        # Fe/H from Fernley+ 1997\n",
    "        fernley97_feh = pd.read_csv(stem + \"fernley_1997_abundances.dat\",delimiter=';')\n",
    "        # RES: 60,000, two FeII lines, 5900-8100 A\n",
    "\n",
    "        # Fe/H from Solano+ 1997\n",
    "        solano_feh = pd.read_csv(stem + \"solano_1997_abundances.dat\",delimiter=';')\n",
    "        # RES: 22,000 & 19,000, strong FeI lines, 4160-4390 & 4070-4490 A\n",
    "        \n",
    "        # Fe/H from Pacino+ 2015\n",
    "        pacino_feh = pd.read_csv(stem + \"pacino_2015_abundances.dat\") \n",
    "        # RES: >30,000, FeI (weighted average), 4000-8500 A\n",
    "\n",
    "        # Fe/H from Sneden+ 2017\n",
    "        sneden_feh = pd.read_csv(stem + \"sneden_2017_abundances.dat\")\n",
    "        # RES: ~27,000 (at 5000 A), FeI & FeII, 3400-9000 A\n",
    "        \n",
    "        # convert Lambert's values, which are in terms of log(eps)\n",
    "        # FeH = log(epsFe) - log(epsFe,sol)\n",
    "        #     = log(epsFe) - log(NFe,sol/NH,sol)\n",
    "        #     = log(epsFe) - 7.51 # value of 7.51 from Anstee+ 1997, MNRAS\n",
    "        lambert_logeps['feh'] = np.subtract(lambert_logeps['log_eps_fe_spec'], 7.51) \n",
    "        # RES: \n",
    "        \n",
    "        # average the values in Chadid from FeI and FeII lines\n",
    "        chadid_feh['feh'] = np.mean([chadid_feh[' fehI'].values,chadid_feh[' fehII'].values],axis=0)\n",
    "        # RES: \n",
    "        \n",
    "        # FYI: average Fe/H values in Liu+ 2013 which were taken at different phases\n",
    "        # liu_feh.groupby(liu_feh['name'], axis=0, as_index=False).mean()\n",
    "        \n",
    "        # FYI: average Fe/H values in Sneden+ 1997 which were taken at different epochs\n",
    "        # sneden_feh.groupby(sneden_feh['name'], axis=0, as_index=False).mean()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # initialize arrays: essential info\n",
    "        empir_spec_name_array = []\n",
    "        star_name_array = []\n",
    "        H_data_array = []\n",
    "        K_data_array = []\n",
    "        err_H_data_array = [] \n",
    "        err_K_data_array = []\n",
    "\n",
    "        # initialize arrays: other info\n",
    "        Hbet_data_array = []\n",
    "        err_Hbet_data_array = []\n",
    "        Hgam_data_array = []\n",
    "        err_Hgam_data_array = []\n",
    "        rHgam_data_array = [] # rescaled Hgamma\n",
    "        err_rHgam_data_array = []\n",
    "        Hdel_data_array = []\n",
    "        err_Hdel_data_array = []\n",
    "        Heps_data_array = []\n",
    "        err_Heps_data_array = []\n",
    "        \n",
    "    def __call__(self):\n",
    "        \n",
    "        # make a list of all UNIQUE, EMPIRICAL spectrum names\n",
    "        uniqueSpecNames = line_data.drop_duplicates(subset='empir_spec_name')['empir_spec_name']\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
